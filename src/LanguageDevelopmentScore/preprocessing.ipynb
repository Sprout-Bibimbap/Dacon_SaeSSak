{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 제작(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_json_files(folder_path):\n",
    "    # Initialize an empty list to store data\n",
    "    data_list = []\n",
    "    \n",
    "    # Walk through all subdirectories and files\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Open and read the JSON file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Append the data to the list\n",
    "                data_list.append(data)\n",
    "    \n",
    "    # Normalize the JSON data\n",
    "    df = pd.json_normalize(data_list, sep='_')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AI챗봇\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"C:\\Users\\jwj51720\\Desktop\\장원준\\Dacon_SaeSSak\\data\\자유대화 음성(소아, 유아)\\Validation\\[라벨]1.AI챗봇\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 음성수집도구\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"C:\\Users\\jwj51720\\Desktop\\장원준\\Dacon_SaeSSak\\data\\자유대화 음성(소아, 유아)\\Validation\\[라벨]2.음성수집도구\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/val_data2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 스튜디오\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"C:\\Users\\jwj51720\\Desktop\\장원준\\Dacon_SaeSSak\\data\\자유대화 음성(소아, 유아)\\Validation\\[라벨]3.스튜디오\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/val_data3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 데이터 Load 및 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/val_data.csv\")\n",
    "df2 = pd.read_csv(\"./data/val_data2.csv\")\n",
    "df3 = pd.read_csv(\"./data/val_data3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1.head(3))\n",
    "display(df2.head(3))\n",
    "display(df3.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.astype({\n",
    "    '음성인식결과': 'string',\n",
    "    '스크립트ID': 'string',\n",
    "    '파일명': 'string',\n",
    "    \"녹음시간\": \"float32\",\n",
    "    \"녹음품질\": \"string\",\n",
    "    \"녹음일시\": \"string\",\n",
    "    \"스크립트셋번호\": \"string\",\n",
    "    \"녹음환경\": \"string\",\n",
    "    \"수집방법\": \"string\",\n",
    "    \"지역\": \"string\",\n",
    "    \"녹음도구\": \"string\",\n",
    "    \"대화주제\": \"string\",\n",
    "    \"성별\": \"string\",\n",
    "    \"녹음자ID\": \"string\",\n",
    "    \"나이\": \"int32\"\n",
    "})\n",
    "df.to_csv(\"./data/validation.csv\", index=False)\n",
    "df.to_parquet(\"./data/validation.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성인식결과는 INPUT\n",
    "print(df[\"음성인식결과\"].value_counts())\n",
    "df[df[\"음성인식결과\"] == \"다음 이 시간 뭐야 음악이야\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역과 성별은 좋은 힌트가 될 수도... 말투의 변화나 관심사의 변호가 있을 수 있음.\n",
    "print(df[\"지역\"].value_counts()) # 수도권에 집중되어 있음\n",
    "print(df[\"성별\"].value_counts()) # 거의 비슷한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 대화주제는 아이가 주제에 얼마나 잘 맞추는지를 평가할 수 있는 지표가 될 수 있음\n",
    "# 집중력? 같은 지표로 제시 가능\n",
    "print(df[\"대화주제\"].value_counts()) # 1201개나 존재\n",
    "\n",
    "def plot_value_counts(value_counts):\n",
    "    # Sort the value counts\n",
    "    sorted_counts = value_counts.sort_values(ascending=False)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(range(len(sorted_counts)), sorted_counts)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Value Counts Distribution')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_value_counts(df[\"대화주제\"].value_counts()) # 특정 주제에 집중되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이는 OUTPUT LABEL\n",
    "df[\"나이\"].value_counts() # 3세 데이터가 많지는 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 데이터 허깅페이스에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"ValidOriginalData.parquet\"\n",
    "\n",
    "# 기존 파일 삭제\n",
    "api.delete_file(\n",
    "    path_in_repo=file_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "# 다시 올리기\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"data/validation.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 데이터만 추출 & 허깅페이스에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"음성인식결과\", \"지역\", \"성별\", \"대화주제\", \"나이\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))\n",
    "print(len(df2.drop_duplicates()))\n",
    "df = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/validation_preprocessing.csv\", index=False)\n",
    "df.to_parquet(\"./data/validation_preprocessing.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"ValidPreprocessedData.parquet\"\n",
    "\n",
    "# 기존 파일 삭제\n",
    "api.delete_file(\n",
    "    path_in_repo=file_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"data/validation_preprocessing.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_path = hf_hub_download(repo_id, \"PreprocessedData.parquet\", repo_type=\"dataset\", use_auth_token=api_key)\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "filtered_df = df[df[\"음성인식결과\"].str.contains(pattern)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters_and_brackets(text):\n",
    "    # 괄호와 괄호 안의 내용을 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # 특수문자 제거 (공백과 알파벳, 숫자, 한글만 남김)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['정제된_결과'] = df['음성인식결과'].apply(remove_special_characters_and_brackets)\n",
    "df[df[\"음성인식결과\"].str.contains(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('', None).dropna(subset=['정제된_결과'])\n",
    "df[df[\"음성인식결과\"].str.contains(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"음성인식결과\"] = df[\"정제된_결과\"]\n",
    "df = df.drop(columns=[\"정제된_결과\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../../data/validation_preprocessing_v2.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"ValidPreprocessedDataClean.parquet\"\n",
    "\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"../../data/validation_preprocessing_v2.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 제작(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    지정된 zip 파일을 열고 모든 파일을 주어진 디렉토리에 압축 해제합니다.\n",
    "    \n",
    "    Args:\n",
    "    zip_path (str): 압축 해제할 zip 파일의 경로.\n",
    "    extract_to (str): 파일을 압축 해제할 대상 폴더의 경로.\n",
    "    \"\"\"\n",
    "    # 해당 디렉토리가 존재하지 않으면 생성\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # Zip 파일 열기\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Zip 내용 압축 해제\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f\"Files extracted to: {extract_to}\")\n",
    "\n",
    "# 사용 예\n",
    "zip_path = 'data/[라벨]1.AI챗봇.zip'\n",
    "extract_to = 'data/[라벨]1.AI챗봇'\n",
    "\n",
    "unzip_file(zip_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"data/[라벨]2.음성수집도구.zip\"\n",
    "extract_to = 'data/[라벨]2.음성수집도구'\n",
    "unzip_file(zip_path, extract_to)\n",
    "\n",
    "zip_path = \"data/[라벨]3.스튜디오.zip\"\n",
    "extract_to = 'data/[라벨]3.스튜디오'\n",
    "unzip_file(zip_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_json_files(folder_path):\n",
    "    # Initialize an empty list to store data\n",
    "    data_list = []\n",
    "    \n",
    "    # Walk through all subdirectories and files\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Open and read the JSON file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Append the data to the list\n",
    "                data_list.append(data)\n",
    "    \n",
    "    # Normalize the JSON data\n",
    "    df = pd.json_normalize(data_list, sep='_')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AI챗봇\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"data/[라벨]1.AI챗봇\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 음성수집도구\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"data/[라벨]2.음성수집도구\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/train_data2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 스튜디오\n",
    "\n",
    "# Example folder path (replace with the actual folder path)\n",
    "folder_path = r\"data/[라벨]3.스튜디오\"\n",
    "\n",
    "# Process the JSON files and create the DataFrame\n",
    "result_df = process_json_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.columns)\n",
    "result_df.columns = [\n",
    "    '음성인식결과',\n",
    "    '스크립트ID',\n",
    "    '파일명',\n",
    "    '녹음시간',\n",
    "    '녹음품질',\n",
    "    '녹음일시',\n",
    "    '스크립트셋번호',\n",
    "    '녹음환경',\n",
    "    '수집방법',\n",
    "    '지역',\n",
    "    '녹음도구',\n",
    "    '대화주제',\n",
    "    '성별',\n",
    "    '녹음자ID',\n",
    "    '나이'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./data/train_data3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 데이터 Load 및 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/train_data.csv\")\n",
    "df2 = pd.read_csv(\"./data/train_data2.csv\")\n",
    "df3 = pd.read_csv(\"./data/train_data3.csv\")\n",
    "\n",
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\n",
    "    '음성인식결과': 'string',\n",
    "    '스크립트ID': 'string',\n",
    "    '파일명': 'string',\n",
    "    \"녹음시간\": \"float32\",\n",
    "    \"녹음품질\": \"string\",\n",
    "    \"녹음일시\": \"string\",\n",
    "    \"스크립트셋번호\": \"string\",\n",
    "    \"녹음환경\": \"string\",\n",
    "    \"수집방법\": \"string\",\n",
    "    \"지역\": \"string\",\n",
    "    \"녹음도구\": \"string\",\n",
    "    \"대화주제\": \"string\",\n",
    "    \"성별\": \"string\",\n",
    "    \"녹음자ID\": \"string\",\n",
    "    \"나이\": \"int32\"\n",
    "})\n",
    "df.to_csv(\"./data/train.csv\", index=False)\n",
    "df.to_parquet(\"./data/train.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역과 성별은 좋은 힌트가 될 수도... 말투의 변화나 관심사의 변호가 있을 수 있음.\n",
    "print(df[\"지역\"].value_counts()) # 수도권에 집중되어 있음\n",
    "print(df[\"성별\"].value_counts()) # 거의 비슷한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"나이\"].value_counts() # 3세 데이터가 많지는 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 허깅페이스에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"TrainOriginalData.parquet\"\n",
    "\n",
    "api.delete_file(\n",
    "    path_in_repo=file_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "# 다시 올리기\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"data/train.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 데이터만 추출 & 허깅페이스에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"음성인식결과\", \"지역\", \"성별\", \"대화주제\", \"나이\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))\n",
    "print(len(df2.drop_duplicates()))\n",
    "df = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/train_preprocessing.csv\", index=False)\n",
    "df.to_parquet(\"./data/train_preprocessing.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"TrainPreprocessedData.parquet\"\n",
    "\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"data/train_preprocessing.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters_and_brackets(text):\n",
    "    # 괄호와 괄호 안의 내용을 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # 특수문자 제거 (공백과 알파벳, 숫자, 한글만 남김)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['정제된_결과'] = df['음성인식결과'].apply(remove_special_characters_and_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('', None).dropna(subset=['정제된_결과'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"음성인식결과\"] = df[\"정제된_결과\"]\n",
    "df = df.drop(columns=[\"정제된_결과\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"./data/train_preprocessing_v2.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"TrainPreprocessedDataClean.parquet\"\n",
    "\n",
    "# api.delete_file(\n",
    "#     path_in_repo=file_in_repo,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=\"dataset\",\n",
    "#     token=api_key\n",
    "# )\n",
    "\n",
    "\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"./data/train_preprocessing_v2.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Trian + valid 결합 및 Test set 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_path = hf_hub_download(repo_id, \"TrainPreprocessedDataClean.parquet\", repo_type=\"dataset\", use_auth_token=api_key)\n",
    "\n",
    "# 파일 로드\n",
    "train = pd.read_parquet(file_path)\n",
    "\n",
    "file_path = hf_hub_download(repo_id, \"ValidPreprocessedDataClean.parquet\", repo_type=\"dataset\", use_auth_token=api_key)\n",
    "\n",
    "# 파일 로드\n",
    "valid = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(valid.head(3))\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, valid], axis = 0)\n",
    "df = df.reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(df[\"나이\"].value_counts().sort_values(key= lambda x: x.index, ascending=False))\n",
    "plt.plot(np.log1p(df[\"나이\"]).value_counts().sort_values(key= lambda x: x.index, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이 3부터 10까지 각각 10개씩 샘플링하여 결합\n",
    "sampled_df_list = []\n",
    "sampled_indices = []\n",
    "\n",
    "for age in range(3, 11):\n",
    "    sampled_df = df[df['나이'] == age].sample(n=10, random_state=3, replace=True)\n",
    "    sampled_df_list.append(sampled_df)\n",
    "    sampled_indices.extend(sampled_df.index)\n",
    "\n",
    "# 샘플링된 데이터프레임 결합\n",
    "result_df = pd.concat(sampled_df_list, ignore_index=True)\n",
    "\n",
    "# 원본 데이터프레임에서 샘플링된 행 제거\n",
    "df_dropped = df.drop(index=sampled_indices)\n",
    "print(len(result_df), len(df_dropped))\n",
    "result_df.to_parquet(\"./data/test.parquet\", index=False)\n",
    "df_dropped.to_parquet(\"./data/all.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"FinalData.parquet\"\n",
    "\n",
    "# 다시 올리기\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"./data/all.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"HF_API_KEY\")\n",
    "api = HfApi()\n",
    "repo_id = \"SaeSSak/Conversation\"\n",
    "file_in_repo = \"TestData.parquet\"\n",
    "\n",
    "# 다시 올리기\n",
    "api.upload_file(\n",
    "        path_or_fileobj=\"./data/test.parquet\", # Local 데이터 경로\n",
    "        path_in_repo=file_in_repo, # Repo에 저장하고자 하는 이름 (parquet 권장)\n",
    "        repo_id=repo_id, # 저장소 이름\n",
    "        repo_type=\"dataset\", # 저장소의 Type\n",
    "        token=api_key # 나의 API 키\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
